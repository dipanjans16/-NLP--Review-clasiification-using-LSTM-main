{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Problem statement**- Conventional optimization functions such as the grid search method  and Bayesian optimization method take a lot of time to optimize the hyperparameters of the LSTM model.  this is primarily because it takes a lot of time to perform the test of accuracy for each architecture and for each learning rate value for a given architecture. This  Project presents a very simple but powerful technique to optimize the hyperparameters in a much shorter time.  \n",
        "\n",
        "**Description**\n",
        "\n",
        "The approach of this method is to optimize the number of layers first without caring about learning rate in stage 1.   in stage 2 we can optimize the model for optimal learning rate for the given number of layers which we selected during stage 1.\n",
        "\n",
        "In this particular example we will try to optimize the LSTM model to predict The positive and negative reviews on a given data set.\n",
        "Here, we will utilize a dataset comprising 50,000 movie reviews from IMDB. Although Keras provides a pre-downloaded dataset that is similar, it is only half the size. However, Keras' version has already undergone a conversion process where the text in the dataset is represented by integer tokens. This conversion is a vital step in natural language processing, which will also be demonstrated in this tutorial. Therefore, we will download the original text data instead of using Keras' preprocessed version."
      ],
      "metadata": {
        "id": "BPSebT4eUQNj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Install the required packages as mentioned below"
      ],
      "metadata": {
        "id": "gTfzpGLYcHBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split as split\n",
        "\n",
        "from keras.layers import LSTM, Input, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.datasets import imdb"
      ],
      "metadata": {
        "id": "ydkoou7XUe9h"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For optimization it is important to define an obective function which can define the criteria of performance.In this case criteria of performance is the accuracy of classification using a certain tnetwork architecture. The network architecture with best accuracy of classification will be selected.\n"
      ],
      "metadata": {
        "id": "7RIPP074UqW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def objective(space):\n",
        "    \n",
        "    num_of_units=int(space[0])\n",
        "    learning_rate1=space[1]\n",
        "    print(space[2])\n",
        "    epocs1=int(space[2])\n",
        "    \n",
        "    \n",
        "    \n",
        "    \"\"\"defining the data parameters\"\"\"\n",
        "    \n",
        "    \n",
        "    \"\"\"num_words sets the maximum number of unique words to be included in the vocabulary, which is used to map words to integers. In this case, the top 88,584 most frequently occurring words in the dataset will be selected.\"\"\"\n",
        "    num_words = 88584\n",
        "    \"\"\"This line loads the IMDB movie review dataset and splits it into training and test sets. The imdb.load_data() function returns a tuple of two lists, where the first list contains the reviews as a sequence of word indices and the second list contains the corresponding sentiment labels (0 for negative and 1 for positive). By setting num_words to num_words, only the top num_words most frequently occurring words are retained in the dataset.\"\"\"\n",
        "    (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = num_words)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    This is prepocessing to trim the length of the sentenses to suible sizes\n",
        "    \n",
        "    The `pad_sequences()` function is used to ensure that all the sequences (i.e., sentences or input data) have the same length. This is important for training neural networks since they require a fixed input size. \n",
        "    \n",
        "    the `pad_sequences()` function is used to pad the sequences in the `train_data` and `test_data` datasets to a maximum length of `max_length`. This ensures that all the sequences in these datasets have the same length, which is necessary to feed them into the neural network model for training and testing.\n",
        "    \n",
        "    The `max_length` parameter specifies the maximum length of the sequences. Any sequences that are shorter than this length are padded with zeros at the end, and any sequences that are longer than this length are truncated.\n",
        "    \n",
        "    After the padding has been applied, the modified sequences are assigned back to the original variables `train_data` and `test_data`, respectively. The modified sequences can then be fed into the neural network for training and testing.\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    max_length= 250\n",
        "    sample_length = 64\n",
        "    import keras\n",
        "    import tensorflow as tf\n",
        "    from keras.utils import pad_sequences\n",
        "    \n",
        "    train_data=pad_sequences(train_data,max_length)\n",
        "    test_data=pad_sequences(test_data,max_length)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    model architecture and optimization\n",
        "    \n",
        "    \n",
        "    This code defines a Sequential model in TensorFlow Keras that can be used for binary classification tasks.\n",
        "    \n",
        "    Here's a breakdown of what each line does:\n",
        "    \n",
        "    model = tf.keras.Sequential([]): This creates a new instance of the Sequential class in TensorFlow Keras, which allows us to stack layers on top of each other to create a neural network.\n",
        "    \n",
        "    tf.keras.layers.Embedding(num_words, num_of_units): This is the first layer in the model. It is an Embedding layer, which takes an integer input (representing the index of a word in a vocabulary) and converts it to a dense vector of fixed size (in this case represented by num_of_units). The num_words argument specifies the size of the vocabulary (i.e., the maximum integer index that can be used as input).\n",
        "    \n",
        "    tf.keras.layers.LSTM(num_of_units): This is the second layer in the model. It is a LSTM layer, which stands for Long Short-Term Memory. LSTM layers are commonly used for processing sequences of data (e.g., text or time-series data). This layer has num_of_units units, which determines the size of the output from this layer.\n",
        "    \n",
        "    tf.keras.layers.Dense(1, activation='sigmoid'): This is the final layer in the model. It is a Dense layer with a single unit, which makes it suitable for binary classification tasks. The sigmoid activation function is used to ensure that the output of the layer is a probability between 0 and 1.\n",
        "    \n",
        "    In summary, this model takes integer inputs (representing words in a vocabulary) and converts them to dense vectors using an Embedding layer. The resulting vectors are then processed by an LSTM layer to capture the sequence information, and finally passed through a Dense layer with a sigmoid activation function to produce a binary classification output.\n",
        "    \"\"\"\n",
        "    \n",
        "    model=tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(num_words,num_of_units),\n",
        "        tf.keras.layers.LSTM(num_of_units),\n",
        "        tf.keras.layers.Dense(1,activation='sigmoid')\n",
        "    ])\n",
        "    \n",
        "    \n",
        "    from keras import optimizers\n",
        "    sgd = optimizers.RMSprop(learning_rate=learning_rate1)\n",
        "    model.compile(loss=\"binary_crossentropy\",optimizer=sgd,metrics=['accuracy'])\n",
        "    history=model.fit(train_data,train_labels,epochs=epocs1,validation_split=0.2)\n",
        "    \n",
        "    \n",
        "    \n",
        "    result=model.evaluate(test_data,test_labels)\n",
        "    print(result)\n",
        "    \n",
        "    return result[1]\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "D54Vg_BVUszh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to define the limitof number of layers we want to iterate over and creating lists for storing the accuracy of each of these models"
      ],
      "metadata": {
        "id": "mBCTw7raarrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maximum_number_of_layers = 200\n",
        "minimum_number_of_layers = 10\n",
        "\n",
        "stage1ac = []  # List to store stage 1 accuracies with respect to change in number of layers\n",
        "Sto_number_of_layers = []  # List to store the number of layers"
      ],
      "metadata": {
        "id": "CS-trrKF-U1K"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iterate over the range of number_of_layers with a step size of 10. Select the optimal number_of_layers based on the maximum value of accuracy."
      ],
      "metadata": {
        "id": "iHe4dYQE-lMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "number_of_epocs = 1\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Iterate over the range of number_of_layers with a step size of 10\n",
        "for number_of_layers in range(minimum_number_of_layers, maximum_number_of_layers, 10):\n",
        "    space = np.array([int(number_of_layers), learning_rate, int(number_of_epocs)])\n",
        "    stage1ac.append(objective(space))  # Call the objective function and append the result to stage1ac\n",
        "    Sto_number_of_layers.append(number_of_layers)  # Append the number_of_layers to Sto_number_of_layers\n",
        "\n",
        "Opt_number_of_layers = Sto_number_of_layers[np.argmax(np.array(stage1ac))]  # Select the optimal number_of_layers based on the maximum value in stage1ac\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSkznacr_Uvd",
        "outputId": "7ee0d632-4856-4841-ca2e-aa8fb9aa8f67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n",
            "625/625 [==============================] - 37s 57ms/step - loss: 0.5228 - accuracy: 0.7641 - val_loss: 0.3634 - val_accuracy: 0.8628\n",
            "782/782 [==============================] - 11s 14ms/step - loss: 0.3689 - accuracy: 0.8552\n",
            "[0.3689042627811432, 0.8551599979400635]\n",
            "1.0\n",
            "625/625 [==============================] - 50s 78ms/step - loss: 0.4283 - accuracy: 0.8055 - val_loss: 0.3516 - val_accuracy: 0.8558\n",
            "782/782 [==============================] - 13s 17ms/step - loss: 0.3583 - accuracy: 0.8553\n",
            "[0.3583487868309021, 0.8553199768066406]\n",
            "1.0\n",
            "625/625 [==============================] - 57s 89ms/step - loss: 0.4167 - accuracy: 0.8109 - val_loss: 0.3411 - val_accuracy: 0.8668\n",
            "782/782 [==============================] - 16s 21ms/step - loss: 0.3466 - accuracy: 0.8599\n",
            "[0.3465568423271179, 0.8598799705505371]\n",
            "1.0\n",
            "625/625 [==============================] - 63s 99ms/step - loss: 0.4062 - accuracy: 0.8151 - val_loss: 0.2920 - val_accuracy: 0.8866\n",
            "782/782 [==============================] - 18s 23ms/step - loss: 0.3036 - accuracy: 0.8784\n",
            "[0.3035806119441986, 0.8784400224685669]\n",
            "1.0\n",
            "625/625 [==============================] - 80s 125ms/step - loss: 0.4165 - accuracy: 0.8140 - val_loss: 0.3167 - val_accuracy: 0.8772\n",
            "782/782 [==============================] - 25s 32ms/step - loss: 0.3458 - accuracy: 0.8592\n",
            "[0.34576472640037537, 0.8592399954795837]\n",
            "1.0\n",
            "625/625 [==============================] - 89s 140ms/step - loss: 0.4206 - accuracy: 0.8100 - val_loss: 0.3551 - val_accuracy: 0.8514\n",
            "782/782 [==============================] - 22s 28ms/step - loss: 0.3572 - accuracy: 0.8512\n",
            "[0.3572029769420624, 0.8512399792671204]\n",
            "1.0\n",
            "625/625 [==============================] - 96s 148ms/step - loss: 0.4250 - accuracy: 0.8112 - val_loss: 0.3653 - val_accuracy: 0.8544\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.3670 - accuracy: 0.8518\n",
            "[0.36699163913726807, 0.8517600297927856]\n",
            "1.0\n",
            "625/625 [==============================] - 101s 160ms/step - loss: 0.4142 - accuracy: 0.8160 - val_loss: 0.3391 - val_accuracy: 0.8628\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 0.3438 - accuracy: 0.8586\n",
            "[0.3437735438346863, 0.8586000204086304]\n",
            "1.0\n",
            "625/625 [==============================] - 115s 182ms/step - loss: 0.4296 - accuracy: 0.8080 - val_loss: 0.3484 - val_accuracy: 0.8668\n",
            "782/782 [==============================] - 33s 42ms/step - loss: 0.3542 - accuracy: 0.8618\n",
            "[0.3541824519634247, 0.8617600202560425]\n",
            "1.0\n",
            "625/625 [==============================] - 140s 221ms/step - loss: 0.4328 - accuracy: 0.8067 - val_loss: 0.2978 - val_accuracy: 0.8786\n",
            "782/782 [==============================] - 36s 46ms/step - loss: 0.3030 - accuracy: 0.8766\n",
            "[0.30295616388320923, 0.8765599727630615]\n",
            "1.0\n",
            "625/625 [==============================] - 174s 275ms/step - loss: 0.4338 - accuracy: 0.8011 - val_loss: 0.3336 - val_accuracy: 0.8624\n",
            "782/782 [==============================] - 44s 56ms/step - loss: 0.3431 - accuracy: 0.8613\n",
            "[0.343137264251709, 0.8612800240516663]\n",
            "1.0\n",
            "625/625 [==============================] - 221s 351ms/step - loss: 0.4330 - accuracy: 0.8072 - val_loss: 0.3899 - val_accuracy: 0.8382\n",
            "782/782 [==============================] - 70s 89ms/step - loss: 0.4046 - accuracy: 0.8272\n",
            "[0.404620498418808, 0.8271600008010864]\n",
            "1.0\n",
            "625/625 [==============================] - 245s 390ms/step - loss: 0.4293 - accuracy: 0.8101 - val_loss: 0.3214 - val_accuracy: 0.8662\n",
            "159/782 [=====>........................] - ETA: 1:03 - loss: 0.3396 - accuracy: 0.8597"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the number of layers has been decided the next step is to decide the learning rate by doing itereatious within defined limits."
      ],
      "metadata": {
        "id": "-adgpmY7__w1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "stage2ac = []  # List to store stage 2 accuracies\n",
        "Sto_number_of_layers = []  # Clear the previous values of Sto_number_of_layers\n",
        "\n",
        "number_of_epocs = 30\n",
        "maximum_learning_rate = 0.02\n",
        "learning_rate = maximum_learning_rate\n",
        "com = 0\n",
        "\n",
        "learning_rate_sto = []  # List to store the learning rate values\n",
        "\n",
        "import math\n",
        "\n",
        "maximum_number_of_iterations = 100\n",
        "iterations = int(math.exp(math.log(maximum_number_of_iterations) / 2)) + 1\n",
        "\n",
        "# Iterate over the range of iterations\n",
        "for itr in range(iterations):\n",
        "    learning_rate_sto.append(learning_rate)  # Append the learning_rate to learning_rate_sto\n",
        "    \n",
        "    space = np.array([int(Opt_number_of_layers), learning_rate, int(number_of_epocs)])\n",
        "    stage2ac.append(objective(space))  # Call the objective function and append the result to stage2ac\n",
        "    \n",
        "    com = stage2ac[itr - 1]\n",
        "    \n",
        "    # Compare the current accuracy with the previous accuracy\n",
        "    if stage2ac[itr] > com:\n",
        "        learning_rate = learning_rate / 2\n",
        "    else:\n",
        "        learning_rate = (maximum_learning_rate + learning_rate) / 2\n",
        "\n",
        "Opt_learning_rate = learning_rate_sto[np.argmax(np.array(stage2ac))]  # Select the optimal learning_rate based on the maximum value in stage2ac"
      ],
      "metadata": {
        "id": "PCqbsoMSAQpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last step is to show the final accuracy of the model"
      ],
      "metadata": {
        "id": "jzittikFBW6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The best model as per defined limits of hyperparameters is\",np.max(np.array(stage2ac)) )"
      ],
      "metadata": {
        "id": "D8RxNI6wBimo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}